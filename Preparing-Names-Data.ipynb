{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Names Data\n",
    "\n",
    "The names data, which is available in the directory ./data/names of this repo, contains a few thousand surnames from 18 languages of origin.  In this notebook I will load that data and split it into train and test sets.  To improve the quality of the classifiers trained on these names it will help to make the training and test datasets balanced across languages.\n",
    "\n",
    "If you are not interested in the details of how I load, balance and split the data, you can safely skip this notebook without loss of continuity since the functions discussed here are also implemented in the [utils module](https://github.com/bobflagg/classifying-names/blob/master/util.py) and in future notebooks I'll just use those methods to load and prepare the data.\n",
    "\n",
    "## Loading the Data\n",
    "\n",
    "Included in the ./data/names directory are 18 text files named as “[Language].txt”. Each file contains one name per line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DIRECTORY = './data/names/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While reading the names, I'll [convert](http://stackoverflow.com/a/518232/2809427) \n",
    "from Unicode to ASCII using the following method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import unicodedata\n",
    "\n",
    "ALL_LETTERS = string.ascii_letters + \" .,;'\"\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in ALL_LETTERS\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following [Sean Robertson](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html?highlight=lstm), I'll first build a dictionary of lists of names per language, {language: [names ...]}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        language   cnt\n",
      " 1.       Arabic: 2000\n",
      " 2.      Chinese:  268\n",
      " 3.        Czech:  519\n",
      " 4.        Dutch:  297\n",
      " 5.      English: 3668\n",
      " 6.       French:  277\n",
      " 7.       German:  724\n",
      " 8.        Greek:  203\n",
      " 9.        Irish:  232\n",
      "10.      Italian:  709\n",
      "11.     Japanese:  991\n",
      "12.       Korean:   94\n",
      "13.       Polish:  139\n",
      "14.   Portuguese:   74\n",
      "15.      Russian: 9408\n",
      "16.     Scottish:  100\n",
      "17.      Spanish:  298\n",
      "18.   Vietnamese:   73\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "language2names = {}\n",
    "languages = []\n",
    "\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "for fname in os.listdir(DIRECTORY): \n",
    "    path = os.path.join(DIRECTORY, fname)\n",
    "    language = os.path.splitext(os.path.basename(fname))[0]\n",
    "    languages.append(language)\n",
    "    names = readLines(path)\n",
    "    language2names[language] = names\n",
    "\n",
    "n_languages = len(languages)\n",
    "print(\"%3s %12s  %4s\" % (\"  \", \"language\", \"cnt\"))\n",
    "for i, language in enumerate(languages):\n",
    "    print(\"%2d. %12s: %4d\" % (i + 1, language, len(language2names[language])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Data\n",
    "\n",
    "I'll use the [scikit-learn](https://scikit-learn.org/stable/index.html)  [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) method to split the samples for each language into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     language   #train    #test\n",
      "      Arabic:     1800      200\n",
      "     Chinese:      241       27\n",
      "       Czech:      467       52\n",
      "       Dutch:      267       30\n",
      "     English:     3301      367\n",
      "      French:      249       28\n",
      "      German:      651       73\n",
      "       Greek:      182       21\n",
      "       Irish:      208       24\n",
      "     Italian:      638       71\n",
      "    Japanese:      891      100\n",
      "      Korean:       84       10\n",
      "      Polish:      125       14\n",
      "  Portuguese:       66        8\n",
      "     Russian:     8467      941\n",
      "    Scottish:       90       10\n",
      "     Spanish:      268       30\n",
      "  Vietnamese:       65        8\n",
      "         All:    18060     2014\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "print(\" %12s   #train    #test\" % (\"language\", ))\n",
    "for i, language in enumerate(languages):\n",
    "    names_train, names_test = train_test_split(\n",
    "        language2names[language], \n",
    "        test_size=0.10, \n",
    "        random_state=42\n",
    "    )\n",
    "    X_train.extend(names_train)\n",
    "    y_train.extend([language] * len(names_train))\n",
    "\n",
    "    X_test.extend(names_test)\n",
    "    y_test.extend([language] * len(names_test))\n",
    "\n",
    "    print(\"%12s: %8d %8d\" % (language, len(names_train), len(names_test)))\n",
    "\n",
    "\n",
    "print(\"%12s: %8d %8d\" % (\"All\", len(X_train), len(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've not got a balanced split of the data into train and test sets but names for the \n",
    "same language are still grouped together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Arabic', 'Arabic', 'Arabic', 'Arabic', 'Arabic', 'Arabic', 'Arabic', 'Arabic']\n",
      "['Arabic', 'Arabic', 'Arabic', 'Arabic', 'Arabic', 'Arabic', 'Arabic', 'Arabic']\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:8])\n",
    "print(y_test[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "This can be easily fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Z = list(zip(X_train, y_train))\n",
    "shuffle(Z)\n",
    "X_train, y_train = zip(*Z)\n",
    "\n",
    "Z = list(zip(X_test, y_test))\n",
    "shuffle(Z)\n",
    "X_test, y_test = zip(*Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Russian', 'German', 'Russian', 'Japanese', 'Russian', 'English', 'Russian', 'Russian')\n",
      "('English', 'Dutch', 'Russian', 'Arabic', 'Czech', 'Czech', 'Japanese', 'Russian')\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:8])\n",
    "print(y_test[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Names\n",
    "\n",
    "To feed the names into a neural network it will be convient to regard a name as a list of characters and to map characters to integers.  For this I'll create a character to integer encoding and encoding and decoding methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_chars: 57\n"
     ]
    }
   ],
   "source": [
    "chars = ALL_LETTERS\n",
    "num_chars = len(chars)\n",
    "\n",
    "print(\"num_chars:\", num_chars)\n",
    "\n",
    "char2int = {ch:ii for ii, ch in enumerate(chars)}\n",
    "int2char = {ii:ch for ch, ii in char2int.items()}\n",
    "\n",
    "def encode_names(some_names):\n",
    "    some_names_encoded = []\n",
    "    for name in some_names: some_names_encoded.append([char2int[c] for c in name])\n",
    "    return some_names_encoded\n",
    "    \n",
    "def decode_name(name): \n",
    "    return \"\".join(int2char[i] for i in name)    \n",
    "\n",
    "def decode_names(some_names): \n",
    "    return [decode_name(name) for name in some_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use these methods to build encoded versions of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_encoded = encode_names(X_train)\n",
    "X_test_encoded = encode_names(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a sanity check to verify the encodings are working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train names sanity check:\n",
      "          Chernovol -->> Russian\n",
      "              Hofer -->> German\n",
      "         Halapkhaev -->> Russian\n",
      "            Tadeshi -->> Japanese\n",
      "             Mokeev -->> Russian\n",
      "        Whittingham -->> English\n",
      "           Tunnikov -->> Russian\n",
      "           Baisarov -->> Russian\n",
      "      Christodoulou -->> Greek\n",
      "            Liharev -->> Russian\n",
      "\n",
      "Test names sanity check:\n",
      "              Eglin -->> English\n",
      "              Simon -->> Dutch\n",
      "             Onikov -->> Russian\n",
      "             Basara -->> Arabic\n",
      "              Rezac -->> Czech\n",
      "            Wykruta -->> Czech\n",
      "           Kimiyama -->> Japanese\n",
      "         Bekmahanov -->> Russian\n",
      "                 Ba -->> Arabic\n",
      "              Tahan -->> Arabic\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "\n",
    "print(\"Train names sanity check:\")\n",
    "X_train_decoded = decode_names(X_train_encoded[:n])\n",
    "for i in range(n):\n",
    "    print(\"%19s -->> %s\" % (X_train_decoded[i], y_train[i]))\n",
    "    \n",
    "print(\"\\nTest names sanity check:\")\n",
    "X_test_decoded = decode_names(X_test_encoded[:n])\n",
    "for i in range(n):\n",
    "    print(\"%19s -->> %s\" % (X_test_decoded[i], y_test[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Names\n",
    "\n",
    "Names are now represented as lists of integers but this is not a convenient representation for our classification algorithms. The fix requires two changes. The first is to represent a single letter as a “one-hot vector” of size <1 x num_chars>. A one-hot vector is filled with 0s except for a 1 at index of the current letter, e.g. \"b\" = <0 1 0 0 0 ...>. With this change the names are represented as lists of one-hot vectors but these lists may have different lengths for different names.  The second patch fixes this by padding the lists when required so that they all have the same length.\n",
    "\n",
    "Let's find the maximum length of a name in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq-length: 19\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "seq_length = np.max([len(name) for name in X_train + X_test])\n",
    "print(\"seq-length: %d\" % seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following methods support representing a list of $n$ names as a tensor with dimension \n",
    "$n$ x seq-length x num-chars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def embed_names(some_names):\n",
    "    some_names_encoded = encode_names(some_names)\n",
    "    some_names_embedded = np.zeros((len(some_names), seq_length, num_chars), dtype=np.float32)\n",
    "    index = 0\n",
    "    for name in some_names_encoded:\n",
    "        position = seq_length - len(name)\n",
    "        for char in name: \n",
    "            some_names_embedded[index, position, char] = 1\n",
    "            position += 1\n",
    "        index += 1\n",
    "    return torch.from_numpy(some_names_embedded) \n",
    "\n",
    "def embed2name(values, indices):\n",
    "    return decode_name([int(indices[i]) for i in range(seq_length) if values[i] == 1])\n",
    "    \n",
    "def embed2names(names):\n",
    "    batch_size = names.shape[0]\n",
    "    values, indices = torch.topk(names, 1)\n",
    "    values = values.squeeze()\n",
    "    indices = indices.squeeze()\n",
    "    return [\n",
    "        embed2name(values[i], indices[i]) for i in range(batch_size)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another sanity check to verify our embedding behaves as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_embedded: torch.Size([18060, 19, 57]) ; torch.float32\n",
      "          Chernovol -->> Russian\n",
      "              Hofer -->> German\n",
      "         Halapkhaev -->> Russian\n",
      "            Tadeshi -->> Japanese\n",
      "             Mokeev -->> Russian\n",
      "        Whittingham -->> English\n",
      "           Tunnikov -->> Russian\n",
      "           Baisarov -->> Russian\n",
      "      Christodoulou -->> Greek\n",
      "            Liharev -->> Russian\n",
      "X_test_embedded: torch.Size([2014, 19, 57]) ; torch.float32\n",
      "              Eglin -->> English\n",
      "              Simon -->> Dutch\n",
      "             Onikov -->> Russian\n",
      "             Basara -->> Arabic\n",
      "              Rezac -->> Czech\n",
      "            Wykruta -->> Czech\n",
      "           Kimiyama -->> Japanese\n",
      "         Bekmahanov -->> Russian\n",
      "                 Ba -->> Arabic\n",
      "              Tahan -->> Arabic\n"
     ]
    }
   ],
   "source": [
    "X_train_embedded = embed_names(X_train)\n",
    "print(\"X_train_embedded:\", X_train_embedded.shape, \";\", X_train_embedded.dtype)\n",
    "\n",
    "n = 10\n",
    "some_names = embed2names(X_train_embedded[:n])\n",
    "for i in range(n):\n",
    "    print(\"%19s -->> %s\" % (some_names[i], y_train[i]))\n",
    "    \n",
    "X_test_embedded = embed_names(X_test)\n",
    "print(\"X_test_embedded:\", X_test_embedded.shape, \";\", X_test_embedded.dtype)\n",
    "\n",
    "n = 10\n",
    "some_names = embed2names(X_test_embedded[:n])\n",
    "for i in range(n):\n",
    "    print(\"%19s -->> %s\" % (some_names[i], y_test[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Labels\n",
    "\n",
    "Feeding lables into a nerual netword will also require an encoding, which is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_labels = list(set(y_train + y_test))\n",
    "distinct_labels.sort()\n",
    "num_classes = len(distinct_labels)\n",
    "label2int = {l:i for i, l in enumerate(distinct_labels)}\n",
    "int2label = {i:l for i, l in enumerate(distinct_labels)}\n",
    "\n",
    "def encode_labels(some_labels): \n",
    "    encoded_labels = np.array([label2int[label] for label in some_labels], dtype=np.float32)\n",
    "    return torch.from_numpy(encoded_labels)\n",
    "\n",
    "def codes2labels(some_labels): \n",
    "    return [int2label[int(i)] for i in some_labels]\n",
    "\n",
    "y_train_encoded = encode_labels(y_train)\n",
    "y_test_encoded = encode_labels(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the expected sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded train labels: torch.Size([18060]) ; torch.float32\n",
      "          Chernovol -->> Russian\n",
      "              Hofer -->> German\n",
      "         Halapkhaev -->> Russian\n",
      "            Tadeshi -->> Japanese\n",
      "             Mokeev -->> Russian\n",
      "        Whittingham -->> English\n",
      "           Tunnikov -->> Russian\n",
      "           Baisarov -->> Russian\n",
      "      Christodoulou -->> Greek\n",
      "            Liharev -->> Russian\n",
      "\n",
      "encoded test labels: torch.Size([2014]) ; torch.float32\n",
      "              Eglin -->> English\n",
      "              Simon -->> Dutch\n",
      "             Onikov -->> Russian\n",
      "             Basara -->> Arabic\n",
      "              Rezac -->> Czech\n",
      "            Wykruta -->> Czech\n",
      "           Kimiyama -->> Japanese\n",
      "         Bekmahanov -->> Russian\n",
      "                 Ba -->> Arabic\n",
      "              Tahan -->> Arabic\n"
     ]
    }
   ],
   "source": [
    "print(\"encoded train labels:\", y_train_encoded.shape, \";\", y_train_encoded.dtype)\n",
    "n = 10\n",
    "some_names = embed2names(X_train_embedded[:n])\n",
    "same_labels = codes2labels(y_train_encoded[:n])\n",
    "for i in range(n):\n",
    "    print(\"%19s -->> %s\" % (some_names[i], same_labels[i]))\n",
    "\n",
    "print(\"\\nencoded test labels:\", y_test_encoded.shape, \";\", y_test_encoded.dtype)\n",
    "n = 10\n",
    "some_names = embed2names(X_test_embedded[:n])\n",
    "same_labels = codes2labels(y_test_encoded[:n])\n",
    "for i in range(n):\n",
    "    print(\"%19s -->> %s\" % (some_names[i], same_labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allennlp",
   "language": "python",
   "name": "allennlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
